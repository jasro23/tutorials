{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-finetune-am-citrinet-tao-deployment/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to deploy a Riva Speech Recognition Pipeline\n",
    "In this tutorial, you will learn how to deploy Riva speech recognition models - specifically the **Acoustic model (Citrinet)**, **Language model (ngram)**, and **Inverse Text Normalization (WSFT)** pre-trained models downloaded from NVIDIA NGC. \n",
    "\n",
    "This will serve as a primer for customization tutorials in this lab, which require configuring the Riva speech pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, ensure that you have access to [**NVIDIA NGC**](https://ngc.nvidia.com/signin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Fetch ASR models from NGC\n",
    "### Download the CitriNet Acoustic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CitriNet Acoustic Model is located on NGC [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_en_us_citrinet/files?version=deployable_v3.0). Let's download it to a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "# Create a local directory to save models\n",
    "ASR_MODEL_DIR = os.path.join(os.getcwd(), \"asr-models\")\n",
    "!mkdir -p $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where ngc will download the Acoustic Model\n",
    "AM_DIR = \"speechtotext_en_us_citrinet_vdeployable_v3.0\"\n",
    "AM_PATH = os.path.join(ASR_MODEL_DIR, AM_DIR)\n",
    "\n",
    "if os.path.exists(AM_PATH):\n",
    "    print(\"Acoustic Model exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the Acoustic Model\")\n",
    "    !ngc registry model download-version \"nvidia/tao/speechtotext_en_us_citrinet:deployable_v3.0\" --dest $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "!ls $AM_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download the n-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n-gram LM is located on NGC [here]( https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_en_us_lm/files?version=deployable_v1.1). \n",
    "\n",
    "`NOTE:` This may take up to 30 minutes to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_DIR = \"speechtotext_en_us_lm_vdeployable_v1.1\"\n",
    "LM_PATH = os.path.join(ASR_MODEL_DIR, LM_DIR)\n",
    "\n",
    "if os.path.exists(LM_PATH):\n",
    "    print(\"Language Model exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the Language Model\")\n",
    "    !ngc registry model download-version \"nvidia/tao/speechtotext_en_us_lm:deployable_v1.1\" --dest $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "!ls $LM_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download Inverse Text Normalization (ITN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ITN model is located on NGC [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/inverse_normalization_en_us/files?version=deployable_v1.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITN_DIR = \"inverse_normalization_en_us_vdeployable_v1.0\"\n",
    "ITN_PATH = os.path.join(ASR_MODEL_DIR, ITN_DIR)\n",
    "\n",
    "if os.path.exists(ITN_PATH):\n",
    "    print(\"ITN Model exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the ITN Model\")\n",
    "    !ngc registry model download-version \"nvidia/tao/inverse_normalization_en_us:deployable_v1.0\" --dest $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "!ls $ITN_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Riva ServiceMaker\n",
    "Riva ServiceMaker is a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components: `riva-build` and `riva-deploy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Itâ€™s only output is an intermediate format (called an RMIR) of an end-to-end pipeline for the supported services within Riva. <br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (`.riva` files) into a single file containing an intermediate format called Riva Model Intermediate Representation (`.rmir`). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. For more information, refer to the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-customizing.html#pipeline-configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"nvcr.io/nvidia/riva/riva-speech:2.4.0-servicemaker\"\n",
    "\n",
    "# Get the ServiceMaker docker\n",
    "! docker pull $RIVA_SM_CONTAINER\n",
    "\n",
    "# Key that model is encrypted with, while exporting with TAO\n",
    "KEY = \"tlt_encode\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we execute Riva-build to create a pipeline configured for Offline Recognition. This command for reference is also present in the [pipeline configuration](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-customizing.html#pipeline-configuration) section of the docs. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set relevant paths relative to where we will mount the models in the Servicemaker docker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model paths relative to Riva Servicemaker docker include the _SM suffix\n",
    "\n",
    "ASR_MODEL_DIR_SM = \"/data\" # Path where we mount the downloaded ASR models in the Servicemaker docker\n",
    "\n",
    "# Relative path to Acoustic Model\n",
    "AM_SM = os.path.join(ASR_MODEL_DIR_SM, AM_DIR, \"citrinet-1024-Jarvis-asrset-3_0-encrypted.riva\")\n",
    "\n",
    "# Relative path to LM model artifacts\n",
    "DECODING_LM_BINARY_SM = os.path.join(ASR_MODEL_DIR_SM, LM_DIR, \"riva_asr_train_datasets_3gram.binary\")\n",
    "DECODING_VOCAB_SM = os.path.join(ASR_MODEL_DIR_SM, LM_DIR, \"flashlight_decoder_vocab.txt\")\n",
    "\n",
    "# Relative path to WSFT artifacts\n",
    "WFST_TOKENIZER_MODEL_SM = os.path.join(ASR_MODEL_DIR_SM, ITN_DIR, \"tokenize_and_classify.far\")\n",
    "WFST_VERBALIZER_MODEL_SM = os.path.join(ASR_MODEL_DIR_SM, ITN_DIR, \"verbalize.far\")\n",
    "\n",
    "# Relative path where the generated .rmir file will be stored\n",
    "ASR_RMIR_SM = os.path.join(ASR_MODEL_DIR_SM, \"asr.rmir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Riva servicemaker docker to run riva-build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run --rm --gpus 0 -v $ASR_MODEL_DIR:$ASR_MODEL_DIR_SM $RIVA_SM_CONTAINER -- \\\n",
    "            riva-build speech_recognition $ASR_RMIR_SM:$KEY $AM_SM:$KEY \\\n",
    "            --name=citrinet-1024-en-US-asr-offline \\\n",
    "            --offline \\\n",
    "            --streaming=False \\\n",
    "            --wfst_tokenizer_model=$WFST_TOKENIZER_MODEL_SM \\\n",
    "            --wfst_verbalizer_model=$WFST_VERBALIZER_MODEL_SM \\\n",
    "            --ms_per_timestep=80 \\\n",
    "            --featurizer.use_utterance_norm_params=False \\\n",
    "            --featurizer.precalc_norm_time_steps=0 \\\n",
    "            --featurizer.precalc_norm_params=False \\\n",
    "            --vad.residue_blanks_at_start=-2 \\\n",
    "            --chunk_size=300 \\\n",
    "            --left_padding_size=0. \\\n",
    "            --right_padding_size=0. \\\n",
    "            --decoder_type=flashlight \\\n",
    "            --flashlight_decoder.asr_model_delay=-1 \\\n",
    "            --decoding_language_model_binary=$DECODING_LM_BINARY_SM  \\\n",
    "            --decoding_vocab=$DECODING_VOCAB_SM  \\\n",
    "            --flashlight_decoder.lm_weight=0.2 \\\n",
    "            --flashlight_decoder.word_insertion_score=0.2 \\\n",
    "            --flashlight_decoder.beam_threshold=20. \\\n",
    "            --language_code=en-US \\\n",
    "            --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments we used above are just an example, and there are many more optional parameter you can configure! For now, let's take a look into what those arguments we used above mean -\n",
    "\n",
    "* General pipeline parameters:\n",
    "    * `--name`: Name of the ASR pipeline, used to set the model names in the Riva model repository\n",
    "    * `--offline`: The Riva ASR pipeline can be configured for both streaming and offline recognition use cases. Here, we mark it to use it in the `offline` setting. More details on recommended configuration for offline/streaming are in the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.html#streaming-offline-recognition).\n",
    "    * `--language_code`: Language of the model\n",
    "    * `--force`: Overwrites the existing .rmir file if it exists.\n",
    "    * `--chunk_size`: Size of audio chunks to use during inference. If not specified, default will be selected based on online/offline setting. \n",
    "    * `--left_padding_size`: The duration in seconds of the backward looking padding to prepend to the audio chunk. The acoustic model input corresponds to a duration of (left_padding_size + chunk_size + right_padding_size) seconds\n",
    "    * `--right_padding_size`: The duration in seconds of the forward looking padding to append to the audio chunk. The acoustic model input corresponds to a duration of (left_padding_size + chunk_size + right_padding_size) seconds\n",
    "* ITN model specific parameters\n",
    "    * `--wfst_tokenizer_model`: Sparrowhawk model to use for tokenization and classification, must be in .far (finite-state archive) format. \n",
    "    * `--wfst_verbalizer_model`: Sparrowhawk model to use for verbalizer, must be in .far (finite-state archive) format.\n",
    "* Acoustic model specific parameters\n",
    "    * `--ms_per_timestep`: The duration in milliseconds of one timestep of the acoustic model output.\n",
    "* Featurizer specific parameters\n",
    "    * `--featurizer.use_utterance_norm_params`: Apply normalization at utterance level\n",
    "    * `--featurizer.precalc_norm_time_steps`: Weight of the precomputed normalization parameters, in timesteps. Setting to 0 will disable use of precalculated normalization parameters.\n",
    "    * `--featurizer.precalc_norm_params`: Boolean that controls if precalculated normalization parameters should be used\n",
    "    * `--vad.residue_blanks_at_start`: Number of time steps to ignore at the beginning of the acoustic model output when trying to detect start/end of speech\n",
    "* Decoder & Language Model specific parameters\n",
    "    * `--decoder_type`: Type of decoder to use. Valid entries are greedy, os2s, flashlight or kaldi. In this example, we used the flashlight decoder.\n",
    "    * `--flashlight_decoder.asr_model_delay`: Number of time steps by which the acoustic model output should be shifted when computing timestamps. This parameter must be tuned since the CTC model is not guaranteed to predict correct alignment.\n",
    "    * `--decoding_language_model_binary`: Language model .binary used during decoding\n",
    "    * `--decoding_vocab`: File of unique words separated by white space. Only used if decoding_lexicon not provided\n",
    "    * `--flashlight_decoder.lm_weight`: Weight of language model. This affects the overall contribution of the language model score to the overall hypothesis score.\n",
    "    * `--flashlight_decoder.word_insertion_score`: Word insertion score used when scoring hypothesis\n",
    "    * `--flashlight_decoder.beam_threshold`: Threshold to prune hypothesis\n",
    "\n",
    "This information is also accessible through the `riva-build speech_recognition -h` command, and more information about additional parameters to `riva-build` can be found in the [riva-build optional parameters](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-custom.html?highlight=riva%20build#riva-build-optional-parameters) documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run --rm $RIVA_SM_CONTAINER -- riva-build speech_recognition -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the .rmir\n",
    "!ls -lt $ASR_MODEL_DIR/*.rmir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more Riva Model Intermediate Representation (RMIR) files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory.\n",
    "\n",
    "`NOTE`: This step may take about 10 mins to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the model repostory relative to the SM docker\n",
    "MODEL_REPO_SM = os.path.join(ASR_MODEL_DIR_SM, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "! docker run --rm --gpus 0 -v $ASR_MODEL_DIR:$ASR_MODEL_DIR_SM $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f  $ASR_RMIR_SM:$KEY $MODEL_REPO_SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $RIVA_SM_CONTAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the models directory\n",
    "!ls -lt $ASR_MODEL_DIR/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start the Riva Server\n",
    "After the model repository is generated, we are ready to start the Riva server. First, download the Riva Skills Quick Start resources from NGC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Riva Skills Quick Start guide\n",
    "The [Riva Skills Quick Start](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart) guide contains easy-to-use scripts to download and deploy models. \n",
    "\n",
    "`NOTE:` The scripts in Quick Start can download and deploy the default models. We downloaded the ASR models above just to demonstrate how to use Riva ServiceMaker tools, which will be used during customization tutorials to re-deploy the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Riva Quick Start directory\n",
    "RIVA_QSG = os.path.join(os.getcwd(), \"riva_quickstart_v2.4.0\")\n",
    "\n",
    "# Downloads the quick start directory to a folder in the current directory and uncompresses it\n",
    "if os.path.exists(RIVA_QSG):\n",
    "    print(\"Riva Quick Start guide exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the Riva Quick Start guide Model\")\n",
    "    !ngc registry resource download-version \"nvidia/riva/riva_quickstart:2.4.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure Riva Quick Start \n",
    "This configures the scripts to deploy the ASR models we obtained as a result of Riva servicemaker tools in the previous section. <br>\n",
    "For this, we modify the `config.sh` file to enable relevant Riva services (ASR for the Citrinet model), provide the encryption key, and path to the model repository (`riva_model_loc`) generated in the previous step among other configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $RIVA_QSG/config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if above the model repository is generated at `$ASR_MODEL_DIR/models`, then you can specify `riva_model_loc` as the same directory as `ASR_MODEL_DIR`. <br>\n",
    "\n",
    "Pretrained versions of models specified in `models_asr/nlp/tts` are fetched from NGC. Since we are using our custom model, we can comment it in `models_asr` (and any others that are not relevant to your use case). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### config.sh snippet\n",
    "```\n",
    "# Enable or Disable Riva Services \n",
    "service_enabled_asr=true\n",
    "service_enabled_nlp=false                                                      ## MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_tts=false                                                     ## MAKE CHANGES HERE - SET TO FALSE\n",
    "\n",
    "# Specify one or more GPUs to use\n",
    "# specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "gpus_to_use=\"device=0\"\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is specified, the data will be written to that location\n",
    "# Otherwise, a docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified. \n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "# \n",
    "# Custom models produced by NeMo or TAO and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"<add path>\"                              ## MAKE CHANGES HERE (Replace with the path ASR_MODEL_DIR)                      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**ATTENTION:**</font> **Make sure to do the following before moving forward:**\n",
    "1. In the file navigator in Jupyter Lab, navigate to riva_quickstart_v2.* and open config.sh\n",
    "2. Configure settings as shown in the snippet above\n",
    "   - Set nlp and tts services to false\n",
    "   - Configure the riva_model_loc path to where the models resulting from riva-deploy are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set `riva-model-loc` to where the models resulting from riva-deploy are stored. In our case it is ASR_MODEL_DIR\n",
    "!echo $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_QSG && chmod +x ./riva_start.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Riva Start to start the server. This will deploy your model(s).\n",
    "! cd $RIVA_QSG && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Riva server is up and running with the models, you can send inference requests querying the server. \n",
    "\n",
    "To send gRPC requests, you can install the Riva Python API bindings for the client. This is available as a `pip` [package](https://pypi.org/project/nvidia-riva-client/). Feel free to read more about the python client [here](https://github.com/nvidia-riva/python-clients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the Client API Bindings\n",
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Riva Server and Run Automatic Speech Recognition\n",
    "The following cells queries the Riva server (using gRPC) with an input audio to yield a transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import riva.client # RIVA 2.3.0 and above\n",
    "except:\n",
    "    import riva_api.riva_audio_pb2 as ra # RIVA 2.0.0 and above\n",
    "    import riva_api.audio_pb2 as ra\n",
    "    import riva_api.riva_asr_pb2 as rasr\n",
    "    import riva_api.riva_asr_pb2_grpc as rasr_srv\n",
    "import wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following URI assumes a local deployment of the Riva Speech API server is on the default port. In case the server deployment is on a different host or via a Helm chart on Kubernetes, use an appropriate URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = riva.client.Auth(uri='localhost:50051')\n",
    "\n",
    "riva_asr = riva.client.ASRService(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample audio file from local disk\n",
    "# This example uses a .wav file with LINEAR_PCM encoding.\n",
    "audio_file = \"audio_samples/en-US_wordboosting_sample1.wav\"\n",
    "    \n",
    "# Listen to the sample audio we are looking to transcribe\n",
    "ipd.Audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = wave.open(audio_file, 'rb')\n",
    "with open(audio_file, 'rb') as fh:\n",
    "    content = fh.read()\n",
    "\n",
    "# Creating RecognitionConfig\n",
    "config = riva.client.RecognitionConfig(\n",
    "  language_code=\"en-US\",\n",
    "  max_alternatives=1,\n",
    "  enable_automatic_punctuation=True,\n",
    "  audio_channel_count = 1\n",
    ")\n",
    "\n",
    "# ASR Inference call with Recognize \n",
    "response = riva_asr.offline_recognize(content, config)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, you should see a transcription result for the input audio sequence. Now you have a speech recognition pipeline running! \n",
    "\n",
    "The ground truth transcription is: *AntiBERTa and ABlooper both transformer based language models are examples of the emerging work in using graph networks to design protein sequences for particular target antigens.*\n",
    "\n",
    "So, it looks like the the domain-specific terms like `AntiBERTa` and `ABlooper` were not transcribed well. In the next notebook, you will look into how you can improve transcription of such words!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

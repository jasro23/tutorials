{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-customize-vocabulary-and-lexicon/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to Customize Riva ASR Vocabulary and Pronunciation with Lexicon Mapping\n",
    "\n",
    "This notebook is walkthough of the process of customizing Riva ASR vocabulary and lexicon, in order to improve Riva vocabulary coverage and recognition of difficult words, such as acronyms.\n",
    "\n",
    "---\n",
    "## Overview\n",
    "\n",
    "The Flashlight decoder, deployed by default in Riva, is a lexicon-based decoder and only emits words that are present in the provided lexicon file. That means, uncommon and new words, such as domain specific terminologies, that are not present in the lexicon file, will have no chance of being generated.\n",
    "\n",
    "On the other hand, the greedy decoder (available as an option during the `riva-build` process with the flag `--decoder_type=greedy`) is not lexicon-based and hence can virtually produce any word or character sequence.\n",
    "\n",
    "---\n",
    "## Pre-requisites\n",
    "\n",
    "This notebook assumes that the user is familiar with manually deploying a Riva ASR pipeline using the Riva ServiceMaker tools, `riva-build` and `riva-deploy`. <br>\n",
    "These were covered in the primer notebook `1_deploy_speech_recognition_pipeline.ipynb` on deploying a speech recognition pipeline. Please run through that notebook as a pre-requisite, and ensure you have the initial Riva ASR pipeline deployed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if your Riva Speech Server is running\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a container with the image `nvcr.io/nvidia/riva/riva-speech:*` running. If not, please execute/re-visit the earlier notebook on deploying a speech recognition pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Terminologies\n",
    "- **Vocabulary file**: The vocabulary file is a flat text file containing a list of vocabulary words, each on its own line. For example:\n",
    "```\n",
    "with\n",
    "not\n",
    "this\n",
    "just\n",
    "my  \n",
    "as  \n",
    "don't\n",
    "...\n",
    "```\n",
    "\n",
    "This file is used by the `riva-build` process to generate the lexicon file. \n",
    "\n",
    "- **Lexicon file**: The lexicon file is a flat text file that contains the mapping of each vocabulary word to its tokenized form, e.g, sentencepiece tokens, separated by a `tab`. Below is an example:\n",
    "\n",
    "```\n",
    "with    ▁with\n",
    "not     ▁not\n",
    "this    ▁this\n",
    "just    ▁just\n",
    "my      ▁my\n",
    "as      ▁as\n",
    "don't   ▁don ' t\n",
    "```\n",
    "\n",
    "*Note: Ultimately, the Riva decoder makes use only of the lexicon file directly at run time (but not the vocabulary file).*\n",
    "\n",
    "Riva Servicemaker automatically tokenizes the words in the vocabulary file to generate the lexicon file. It uses the correct tokenizer model that is packaged together with the acoustic model in the `.riva` file. By default, Riva generates 1 tokenized form for each word in the vocabulary file. You will learn more about finetuning the acoustic model and the .riva format in subsequent notebooks in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What can be customized?\n",
    "\n",
    "Both the vocabulary and the lexicon files can be customized.\n",
    "\n",
    "1. **Extending the vocabulary** enriches the Riva default vocabulary, providing additional coverage for out-of-vocabulary words, terminologies, and abbreviations.\n",
    "\n",
    "2. **Customizing the lexicon file** can further enrich the Riva knowledge base by providing one or more explicit pronunciations, in the form of tokenized sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Extending the vocabulary\n",
    "\n",
    "Extending the vocabulary must be done at Riva **build** time.\n",
    "\n",
    "When building a Riva ASR pipeline, pass the [extended vocabulary file](#modify_vocab) to the `--decoding_vocab=<vocabulary_file>` parameter of the build command. For example, the build command for the Citrinet model:\n",
    "\n",
    "```\n",
    "    riva-build speech_recognition \\\n",
    "   <rmir_filename>:<key> <riva_filename>:<key> \\\n",
    "   --name=citrinet-1024-english-asr-streaming \\\n",
    "   --decoding_language_model_binary=<lm_binary> \\ \n",
    "   --decoding_vocab=<vocabulary_file> \\                          ## PASS THE MODIFIED VOCABULARY FILE HERE\n",
    "   --language_code=en-US \\\n",
    "   <other_parameters>...\n",
    "```\n",
    "\n",
    "Refer to Riva [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/service-asr.html#pipeline-configuration) for build commands for supported models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modify_vocab'></a>\n",
    "### How to modify vocabulary file\n",
    "\n",
    "You can either provide your own vocabulary file, or extend Riva's default vocabulary file.\n",
    "\n",
    "- **BYO vocabulary file**: provide a flat text file containing a list of vocabulary words, each on its own line. Note that this file must not only contain a small list of \"difficult words\", but must contains all the words that you want the ASR pipeline to be able to generate, that is, including all common words.\n",
    "\n",
    "- **Modifying an existing vocabulary**: This is the recommended approach. Out-of-the-box vocabulary files  for Riva supported languages can be found either:\n",
    "    1. **On NGC**\n",
    "    2. Or **In a local Riva deployment**\n",
    "\n",
    "You can make a copy, then extend these default vocabulary files with the words of interest. Let's take a look at different options for **Modifying an existing vocabulay:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Option 1. Modifying an existing vocab - Vocabulary file from NGC\n",
    "\n",
    "**On NGC**, for example, for English, the vocabulary file named `flashlight_decoder_vocab.txt` can be found at this [link](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_en_us_lm/files?version=deployable_v1.1).\n",
    "\n",
    "We had downloaded deployable ASR models, including the [Language Model](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_en_us_citrinet/files?version=deployable_v3.0) from NGC in `ASR_MODEL_DIR` in the first notebook in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the ASR models\n",
    "ASR_MODEL_DIR = os.path.join(os.getcwd(), \"asr-models\")\n",
    "\n",
    "# See what the decoder vocab file looks like\n",
    "!tail -n 5 $ASR_MODEL_DIR/speechtotext_en_us_lm_vdeployable_v1.1/flashlight_decoder_vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Option 2.  Modifying an existing vocab - Vocabulary file from local Riva deployment\n",
    "The actual physical location of Riva assets depends on the value of the `riva_model_loc` variable in the `config.sh` file in the Riva quickstart folder. The vocabulary file is bundled with the Flashlight decoder.\n",
    "-  By default, `riva_model_loc` is set to `riva-model-repo`, which is a docker volume. You can inspect this docker volume and copy the vocabulary file from within the docker volume to the host file system with commands such as:\n",
    "        \n",
    "        ```bash\n",
    "        # Inspect the Riva model docker volume\n",
    "        docker inspect riva-model-repo\n",
    "        \n",
    "        # Inspect the content of the Riva model docker volume\n",
    "        docker run --rm -v riva-model-repo:/riva-model-repo alpine ls /riva-model-repo\n",
    "        \n",
    "        # Copy the vocabulary file from the docker volume to the current directory\n",
    "        docker run --rm -v $PWD:/dest -v riva-model-repo:/riva-model-repo alpine cp  /riva-model-repo/models/citrinet-1024-en-US-asr-offline-ctc-decoder-cpu-offline/1/dict_vocab.txt /dest\n",
    "        ```\n",
    "        \n",
    "- If you modify `riva_model_loc` to an absolute path pointing to a folder, then the specified folder in the local file system will be used to store Riva assets instead. Assuming `ASR_MODEL_DIR` is the directory where Riva assets are stored, then the vocabulary file can similarly be found under, for example, `$ASR_MODEL_DIR/models/citrinet-1024-en-US-asr-offline-ctc-decoder-cpu-offline/1/dict_vocab.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Riva Quickstart - this was downloaded as part of the first notebook\n",
    "RIVA_QSS_DIR = os.path.join(os.getcwd(), \"riva_quickstart_v2.3.0\")\n",
    "\n",
    "# In the first notebook, we modified our config.sh to point to an absolute path, where the riva-deploy command stored the ASR model\n",
    "!sed -n 58,64p $RIVA_QSS_DIR/config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make a copy, then extend this default vocabulary file with the words of interest.\n",
    "\n",
    "<font color='red'>**ATTENTION:**</font> **Once modified, you'll have to redeploy the Riva ASR pipeline with `riva-build` while passing the flag `--decoding_vocab=<modified_vocabulary_file>`**. <br>\n",
    "\n",
    "For the purposes of this notebook, we will execute the workflow of the next section - customizing pronunciation with lexicon mapping, which is similar enough in terms of pipeline re-deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 2. Customizing pronunciation with lexicon mapping\n",
    "\n",
    "The lexicon file that is used by the Flashlight decoder can be found in the Riva assets directory, as specified by the value of the `riva_model_loc` variable in the `config.sh` file under the Riva quickstart folder.\n",
    "\n",
    "- If `riva_model_loc` points to a docker volume (by default), you can find and copy the lexicon file with:\n",
    "```bash\n",
    "        # Copy the lexicon file from the docker volume to the current directory\n",
    "        docker run --rm -v $PWD:/dest -v riva-model-repo:/riva-model-repo alpine cp  /riva-model-repo/models/citrinet-1024-en-US-asr-offline-ctc-decoder-cpu-offline/1/lexicon.txt /dest\n",
    "```      \n",
    "<br>\n",
    "- If you modify `riva_model_loc` to an absolute path pointing to a folder, then the specified folder in the local file system will be used to store Riva assets instead. Assuming `ASR_MODEL_DIR` is the directory where Riva assets are stored, then the vocabulary file can similarly be found under, for example, `$ASR_MODEL_DIR/models/citrinet-1024-en-US-asr-offline-ctc-decoder-cpu-offline/1/lexicon.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASR_MODEL_DIR = os.path.join(os.getcwd(), \"asr-models\")\n",
    "\n",
    "!tail -5 $ASR_MODEL_DIR/models/citrinet-1024-en-US-asr-offline-ctc-decoder-cpu-offline/1/lexicon.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How to generate the correct tokenized form\n",
    "\n",
    "When modifying the lexicon file, ensure that:\n",
    "\n",
    "- The new lines follow the indentation/space pattern like the rest of the file and that the tokens used are part of the tokenizer model. \n",
    "\n",
    "- The tokens are valid tokens as determined by the tokenizer model (packaged with the Riva acoustic model).\n",
    "\n",
    "The latter ensures that you use only tokens that the acoustic model has been trained on. To do this, you’ll need the **tokenizer model** and the **[Sentencepiece](https://github.com/google/sentencepiece)** Python package (`pip install sentencepiece`). <br>\n",
    "\n",
    "You can get the tokenizer model for the deployed pipeline from one of the below locations:\n",
    "\n",
    "1. The model repository `ctc-decoder-...` directory for your model. It will be named `<hash>_tokenizer.model`. For example:\n",
    "\n",
    "`<ASR_MODEL_DIR>/models/citrinet-1024-en-US-asr-streaming-ctc-decoder-cpu-offline/1/498056ba420d4bb3831ad557fba06032_tokenizer.model`\n",
    "\n",
    "2. When using a docker volume to store Riva assets (by default), you can copy the tokenizer model to the local directory with a command such as:\n",
    "\n",
    "```bash\n",
    "    # Copy the tokenizer model file from the docker volume to the current directory\n",
    "    docker run --rm -v $PWD:/dest -v riva-model-repo:/riva-model-repo alpine cp  /riva-model-repo/models/citrinet-1024-en-US-asr-streaming-ctc-decoder-cpu-offline/1/498056ba420d4bb3831ad557fba06032_tokenizer.model /dest\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first copy the tokenizer from the previously running instance of Riva server to current directory\n",
    "!docker cp riva-speech:/data/models/citrinet-1024-en-US-asr-offline-ctc-decoder-cpu-offline/1/498056ba420d4bb3831ad557fba06032_tokenizer.model {ASR_MODEL_DIR}\n",
    "\n",
    "TOKENIZER_MODEL = os.path.join(ASR_MODEL_DIR, \"498056ba420d4bb3831ad557fba06032_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then generate new lexicon entries as shown below. The pronunciation you choose gets tokenized by the tokenizer model, and this will be what will eventually be detected to get the word in transcription.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN=\"antiberta\" # Antiberta\n",
    "PRONUNCIATION=\"anti berta\"\n",
    "\n",
    "s = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL)\n",
    "\n",
    "# Enabling sampling can help sample several possible segmentations\n",
    "for n in range(5):\n",
    "    print(TOKEN + '\\t' + ' '.join(s.encode(PRONUNCIATION, out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN=\"ablooper\" # and ABlooper\n",
    "PRONUNCIATION=\"a blooper\"\n",
    "\n",
    "s = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL)\n",
    "\n",
    "for n in range(5):\n",
    "    print(TOKEN + '\\t' + ' '.join(s.encode(PRONUNCIATION, out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### How to modify the lexicon file\n",
    "\n",
    "First, locate and make a copy of the lexicon file. For example:\n",
    "```\n",
    "cp <ASR_MODEL_DIR>/models/citrinet-1024-en-US-asr-offline-ctc-decoder-cpu-offline/1/lexicon.txt modified_lexicon.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $ASR_MODEL_DIR/models/citrinet-1024-en-US-asr-offline-ctc-decoder-cpu-offline/1/lexicon.txt $ASR_MODEL_DIR/modified_lexicon.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 5 $ASR_MODEL_DIR/modified_lexicon.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, modify it to add the sentencepiece tokenizations for the words of interest. We choose one of the sampled tokenizations for each of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's append tokenized antiberta and ablooper following the convention in the lexicon file\n",
    "!echo -e \"antiberta\\t▁an t i b er ta\" >> $ASR_MODEL_DIR/modified_lexicon.txt\n",
    "!echo -e \"ablooper\\t▁a b lo op er\" >> $ASR_MODEL_DIR/modified_lexicon.txt\n",
    "\n",
    "# Verify that words of interest have been successfully appended\n",
    "!tail -n 5 $ASR_MODEL_DIR/modified_lexicon.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once this is done, we need to regenerate the model repository using that new decoding lexicon tokenization by passing `--decoding_lexicon=/path/to/modified_lexicon.txt` to `riva-build` instead of `--decoding_vocab=/path/to/decoding_vocab.txt`. <br>\n",
    "Let's now put these ideas to action in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Default Inference\n",
    "First, let's try to transcribe using the pipeline without any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "\n",
    "import riva.client\n",
    "\n",
    "auth = riva.client.Auth(uri='localhost:50051')\n",
    "riva_asr = riva.client.ASRService(auth)\n",
    "\n",
    "# Load a sample audio file from local disk\n",
    "# This example uses a .wav file with LINEAR_PCM encoding.\n",
    "path = \"audio_samples/en-US_wordboosting_sample1.wav\"\n",
    "with io.open(path, 'rb') as fh:\n",
    "    content = fh.read()\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RecognitionConfig\n",
    "config = riva.client.RecognitionConfig(\n",
    "  language_code=\"en-US\",\n",
    "  max_alternatives=1,\n",
    "  enable_automatic_punctuation=True,\n",
    "  audio_channel_count = 1\n",
    ")\n",
    "\n",
    "# ASR Inference call with Recognize \n",
    "response = riva_asr.offline_recognize(content, config)\n",
    "asr_best_transcript = response.results[0].alternatives[0].transcript\n",
    "print(\"ASR Transcript without Word Boosting:\", asr_best_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the unmodified pipeline, ASR is having a hard time recognizing domain specific terms like `AntiBERTa` and `ABlooper`. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Riva-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the existing server\n",
    "RIVA_QSS_DIR = os.path.join(os.getcwd(), \"riva_quickstart_v2.3.0\")\n",
    "\n",
    "! cd $RIVA_QSS_DIR && chmod +x riva_stop.sh\n",
    "! cd $RIVA_QSS_DIR && ./riva_stop.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the Riva Speech Server has stopped. No containers should be running at this point.\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"nvcr.io/nvidia/riva/riva-speech:2.3.0-servicemaker\"\n",
    "\n",
    "# Get the ServiceMaker docker\n",
    "! docker pull $RIVA_SM_CONTAINER\n",
    "\n",
    "# Default key that model is encrypted with\n",
    "KEY = \"tlt_encode\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riva-build for Offline Recognition usecase. Reference: [Pipeline configuration](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-customizing.html#pipeline-configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set relevant paths relative to where we will mount the models in the Servicemaker docker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model paths relative to Riva Servicemaker docker include the _SM suffix\n",
    "\n",
    "ASR_MODEL_DIR_SM = \"/data\" # Path where we mount the downloaded ASR models in the Servicemaker docker\n",
    "\n",
    "# Relative path to Acoustic Model\n",
    "AM_SM = os.path.join(ASR_MODEL_DIR_SM, \"speechtotext_en_us_citrinet_vdeployable_v3.0\", \"citrinet-1024-Jarvis-asrset-3_0-encrypted.riva\")\n",
    "\n",
    "# Relative path to LM model artifacts\n",
    "DECODING_LM_BINARY_SM = os.path.join(ASR_MODEL_DIR_SM, \"speechtotext_en_us_lm_vdeployable_v1.1\", \"riva_asr_train_datasets_3gram.binary\")\n",
    "\n",
    "# Relative path to WSFT artifacts\n",
    "WFST_TOKENIZER_MODEL_SM = os.path.join(ASR_MODEL_DIR_SM, \"inverse_normalization_en_us_vdeployable_v1.0\", \"tokenize_and_classify.far\")\n",
    "WFST_VERBALIZER_MODEL_SM = os.path.join(ASR_MODEL_DIR_SM, \"inverse_normalization_en_us_vdeployable_v1.0\", \"verbalize.far\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**ATTENTION:**</font> We will provide `--decoding_lexicon=/path/to/modified_lexicon.txt` to `riva-build` instead of `--decoding_vocab=/path/to/flashlight_decoder_vocab.txt`, through `DECODING_LEXICON_SM` declared below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the modified lexicon file instead of the default vocab\n",
    "DECODING_LEXICON_SM = os.path.join(ASR_MODEL_DIR_SM, \"modified_lexicon.txt\") \n",
    "\n",
    "# Relative path where the generated .rmir file will be stored, we indicate that this is with the modified lexicon\n",
    "ASR_RMIR_SM = os.path.join(ASR_MODEL_DIR_SM, \"asr-mod-lexicon.rmir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Riva servicemaker docker to run riva-build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will create a \"asr-mod-lexicon.rmir\"\n",
    "! docker run --rm --gpus 0 -v $ASR_MODEL_DIR:$ASR_MODEL_DIR_SM $RIVA_SM_CONTAINER -- \\\n",
    "            riva-build speech_recognition $ASR_RMIR_SM:$KEY $AM_SM:$KEY \\\n",
    "            --name=citrinet-1024-en-US-asr-offline \\\n",
    "            --offline \\\n",
    "            --streaming=False \\\n",
    "            --wfst_tokenizer_model=$WFST_TOKENIZER_MODEL_SM \\\n",
    "            --wfst_verbalizer_model=$WFST_VERBALIZER_MODEL_SM \\\n",
    "            --ms_per_timestep=80 \\\n",
    "            --featurizer.use_utterance_norm_params=False \\\n",
    "            --featurizer.precalc_norm_time_steps=0 \\\n",
    "            --featurizer.precalc_norm_params=False \\\n",
    "            --vad.residue_blanks_at_start=-2 \\\n",
    "            --chunk_size=300 \\\n",
    "            --left_padding_size=0. \\\n",
    "            --right_padding_size=0. \\\n",
    "            --decoder_type=flashlight \\\n",
    "            --flashlight_decoder.asr_model_delay=-1 \\\n",
    "            --decoding_language_model_binary=$DECODING_LM_BINARY_SM  \\\n",
    "            --decoding_lexicon=$DECODING_LEXICON_SM \\\n",
    "            --flashlight_decoder.lm_weight=0.2 \\\n",
    "            --flashlight_decoder.word_insertion_score=0.2 \\\n",
    "            --flashlight_decoder.beam_threshold=20. \\\n",
    "            --language_code=en-US \\\n",
    "            --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run `riva-deploy` to generate the model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "! docker run --rm --gpus 0 -v $ASR_MODEL_DIR:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f  $ASR_RMIR_SM:$KEY /data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-deploy the speech recognition pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd $RIVA_QSS_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trying the sample again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying again\n",
    "auth = riva.client.Auth(uri='localhost:50051')\n",
    "riva_asr = riva.client.ASRService(auth)\n",
    "\n",
    "path = \"audio_samples/en-US_wordboosting_sample1.wav\"\n",
    "with io.open(path, 'rb') as fh:\n",
    "    content = fh.read()\n",
    "    \n",
    "# Creating RecognitionConfig\n",
    "config = riva.client.RecognitionConfig(\n",
    "  language_code=\"en-US\",\n",
    "  max_alternatives=1,\n",
    "  enable_automatic_punctuation=True,\n",
    "  audio_channel_count = 1\n",
    ")\n",
    "\n",
    "# ASR Inference call with Recognize \n",
    "response = riva_asr.offline_recognize(content, config)\n",
    "asr_best_transcript = response.results[0].alternatives[0].transcript\n",
    "print(\"ASR Transcript without Word Boosting:\", asr_best_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `AntiBerta` and `ABlooper` are being transcribed correctly after customizing the lexicon. <br>\n",
    "To recap, customizing the lexicon can help extend the vocabulary, and also allow us to provide one or more custom pronunciations for words explicitly for better recognition. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
